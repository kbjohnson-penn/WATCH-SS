{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07ddeebb-39d4-457f-9e90-7d76609e4f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Penn Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "139033ad-a3b5-4a80-9896-84f9221516db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport data.observer\n",
    "%aimport utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1c1fd7-7c4d-454f-9163-894718b80585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import data.observer as OBSERVER\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06b668c8-7d83-4c29-8466-af7c8c75eb08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "273a3cb3-4d7d-4290-990c-116dce6e2eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Cognitive Impairment Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0abb01c0-c46f-47b9-ba75-a56421725104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labels = pd.read_excel(\"/Volumes/biomedicalinformatics_analytics/dev_lab_johnson/watch/penn_CI_labels.xlsx\")\n",
    "labels[\"patient_id\"] = labels[\"patient_id\"].str.strip()\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a575418-5c14-4aa7-bd82-e10fcc943f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ecd012-63f8-4153-ac7d-0b3b5bd24a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"%d & %d\\\\%% & %.1f $\\\\pm$ %.1f & %d & %d\\\\%% & %.1f $\\\\pm$ %.1f \\\\\\\\\" %\n",
    "      ((labels[\"CI\"] == 1).sum(),\n",
    "        100 * ((labels[\"CI\"] == 1) & (labels[\"Gender\"] == \"Female\")).sum() / (labels[\"CI\"] == 1).sum(),\n",
    "        labels.loc[labels[\"CI\"] == 1, \"Age (in Years)\"].mean(),\n",
    "        labels.loc[labels[\"CI\"] == 1, \"Age (in Years)\"].std(),\n",
    "       (labels[\"CI\"] == 0).sum(),\n",
    "        100 * ((labels[\"CI\"] == 0) & (labels[\"Gender\"] == \"Female\")).sum() / (labels[\"CI\"] == 0).sum(),\n",
    "        labels.loc[labels[\"CI\"] == 0, \"Age (in Years)\"].mean(),\n",
    "        labels.loc[labels[\"CI\"] == 0, \"Age (in Years)\"].std()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75328ea5-93f8-49ef-bda5-a551028c2889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Downsampling the cognitively normal cohort to be balanced with the cognitively impaired cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54371ab8-8cc9-4f1f-9716-1782bdb52919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234567)\n",
    "\n",
    "ci_mean_age = labels.loc[labels[\"CI\"] == 1, \"Age (in Years)\"].mean()\n",
    "ci_pct_female = 100 * ((labels[\"CI\"] == 1) & (labels[\"Gender\"] == \"Female\")).sum() / (labels[\"CI\"] == 1).sum()\n",
    "\n",
    "cn_patients = labels.loc[labels[\"CI\"] == 0, \"patient_id\"]\n",
    "\n",
    "results = []\n",
    "for i in range(10000):\n",
    "    sample = np.random.choice(cn_patients, size=(labels[\"CI\"] == 1).sum(), replace=False)\n",
    "    results.append({\n",
    "        \"pt_ids\": sample, \n",
    "        \"avg_age\": labels.loc[labels[\"patient_id\"].isin(sample), \"Age (in Years)\"].mean(),\n",
    "        \"pct_female\": 100 * (labels[\"patient_id\"].isin(sample) & (labels[\"Gender\"] == \"Female\")).sum() / sample.shape[0]\n",
    "    })\n",
    "    \n",
    "candidate_subsamples = pd.DataFrame(results)\n",
    "candidate_subsamples[\"mean_age_diff\"] = abs(ci_mean_age - candidate_subsamples[\"avg_age\"])\n",
    "candidate_subsamples[\"pct_female_diff\"] = abs(ci_pct_female - candidate_subsamples[\"pct_female\"])\n",
    "candidate_subsamples[\"composite_diff\"] = candidate_subsamples[\"mean_age_diff\"] + candidate_subsamples[\"pct_female_diff\"]\n",
    "pt_subsample = candidate_subsamples.loc[candidate_subsamples[\"composite_diff\"].idxmin(), \"pt_ids\"]\n",
    "pt_subsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d32e98fe-0b45-443c-a54c-a7de14838974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "'PT025', 'PT099', 'PT075', 'PT016', 'PT044', 'PT085', 'PT035', 'PT103', 'PT036', 'PT058', 'PT002', 'PT095'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c726f5-6538-4cbe-9416-48858fa5a35c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labels_ds = labels.loc[(labels[\"CI\"] == 1) | labels[\"patient_id\"].isin(pt_subsample)]\n",
    "\n",
    "print(\"%d & %d\\\\%% & %.1f $\\\\pm$ %.1f & %d & %d\\\\%% & %.1f $\\\\pm$ %.1f \\\\\\\\\" %\n",
    "      ((labels_ds[\"CI\"] == 1).sum(),\n",
    "        100 * ((labels_ds[\"CI\"] == 1) & (labels_ds[\"Gender\"] == \"Female\")).sum() / (labels_ds[\"CI\"] == 1).sum(),\n",
    "        labels_ds.loc[labels_ds[\"CI\"] == 1, \"Age (in Years)\"].mean(),\n",
    "        labels_ds.loc[labels_ds[\"CI\"] == 1, \"Age (in Years)\"].std(),\n",
    "       (labels_ds[\"CI\"] == 0).sum(),\n",
    "        100 * ((labels_ds[\"CI\"] == 0) & (labels_ds[\"Gender\"] == \"Female\")).sum() / (labels_ds[\"CI\"] == 0).sum(),\n",
    "        labels_ds.loc[labels_ds[\"CI\"] == 0, \"Age (in Years)\"].mean(),\n",
    "        labels_ds.loc[labels_ds[\"CI\"] == 0, \"Age (in Years)\"].std()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d2436c1-5f23-4ae0-9007-549ef060bd83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Cognitive Test Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d655ba3e-bddd-4b1a-86c3-3665a2cda4d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lbls = OBSERVER.load_labels()\n",
    "lbls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63298b4e-5847-4c35-a5ae-2d70ad2c3c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 30, 15)\n",
    "\n",
    "plt.hist(lbls[\"MMSE\"], bins=bins, edgecolor=\"k\", zorder=3)\n",
    "plt.xlabel(\"MMSE\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim([-1, 31])\n",
    "plt.grid(zorder=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5030e3b-8728-4d64-bcda-11dd257a0d08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 30, 15)\n",
    "\n",
    "plt.hist(lbls[\"FRS\"], bins=bins, edgecolor=\"k\", zorder=3)\n",
    "plt.xlabel(\"FRS\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim([-1, 31])\n",
    "plt.grid(zorder=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccf18a78-2c8b-4ff4-9c9b-13da472c5f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Labeling Probable MCI using GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e137a06-47ce-4b04-8f59-f6134d0360d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0c95fe-607d-4ced-93a6-34c2e7721729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import tiktoken\n",
    "import time\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0ed324c-34a8-4fa2-a303-a2c25ec41fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e979dc-04e3-4961-b3b2-359430d5967c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "directory = \"/Volumes/biomedicalinformatics_analytics/dev_lab_johnson/swimcap/Penn OBSERVER/problem_lists/\"\n",
    "\n",
    "idx, pls = [], []\n",
    "for file in os.listdir(directory):\n",
    "    with open(os.path.join(directory, file), \"r\") as fp:\n",
    "        idx.append(file.rsplit(\".\")[0])\n",
    "        pls.append(fp.read())\n",
    "\n",
    "problem_lists = pd.DataFrame(data=pls, index=idx, columns=[\"problem_list\"])\n",
    "problem_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2debc1-a568-4c8a-a66a-d541a3b90039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using o200k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0125\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        \"gpt-4o-mini-2024-07-18\",\n",
    "        \"gpt-4o-2024-08-06\"\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0125\")\n",
    "    elif \"gpt-4o-mini\" in model:\n",
    "        print(\"Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\")\n",
    "    elif \"gpt-4o\" in model:\n",
    "        print(\"Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4o-2024-08-06\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc4260e-e8f7-43e6-80ea-23dc824bb0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    base_url=\"https://adb-2035410508966251.11.azuredatabricks.net/serving-endpoints\"\n",
    ")\n",
    "\n",
    "model   = \"openai_gpt_4o\"\n",
    "tpm     = 1e6\n",
    "rpm     = 6150\n",
    "\n",
    "prompt  = (\n",
    "    \"Here is a patient's problem list summarizing their active health issues \"\n",
    "    \"(e.g., diagnoses, chronic conditions, injuries):\\n\\n\"\n",
    "    \"{}\\n\\n\"\n",
    "    \"Based on this information, label the patient as either:\\n\\n\"\n",
    "    \"- Probable MCI (Mild Cognitive Impairment), or\\n\"\n",
    "    \"- Healthy Control\\n\\n\"\n",
    "    \"If the problem list includes multiple conditions that are commonly associated \"\n",
    "    \"with cognitive decline, MCI, or Alzheimer's dementia (e.g., memory loss, gait abnormality), consider assigning \"\n",
    "    'the \\\"Probable MCI\\\" label. Otherwise, assign \\\"Healthy Control\\\".\\n\\n'\n",
    "    \"Format your response as follows:\\n\\n\"\n",
    "    \"- Label: <Probable MCI or Healthy Control>\\n\"\n",
    "    \"- Reason: <comma-separated list of relevant issues from the problem list, or \\\"N/A\\\" if Healthy Control>\"\n",
    ")\n",
    "\n",
    "\n",
    "tokens_used = 0\n",
    "t = time.time()\n",
    "for i, row in problem_lists.iterrows():\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt.format(row.problem_list)}]\n",
    "    n_query_tokens = num_tokens_from_messages(messages, model.split(\"_\", maxsplit=1)[1].replace(\"_\", \"-\"))\n",
    "    print(\"N query tokens:\", n_query_tokens)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "    n_response_tokens = response.usage.completion_tokens\n",
    "    print(\"N response tokens:\", n_response_tokens)\n",
    "\n",
    "    tokens_used += n_response_tokens\n",
    "\n",
    "    matches = re.findall(r'^\\s*[^:]+:\\s*(.*)', output, re.MULTILINE)\n",
    "    problem_lists.loc[i, \"generated_label\"] = matches[0].rstrip()\n",
    "    problem_lists.loc[i, \"reason\"] = matches[1].rstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1cef09-1bb0-43d0-bb2c-88c55ce49e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "problem_lists.to_excel(\"visit_problem_lists_labeled.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc9d39fa-ccdc-44d2-affd-6511874fec16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8405ae9-d7e3-49da-8a3f-22f9ca6eb0eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from detectors.filler_speech.keyword_search import FillerKeywordDetector\n",
    "from detectors.repetitive_speech.unigram_analysis import UnigramAnalysisDetector as WordRepetitionDetector\n",
    "from detectors.vague_speech.keyword_search import VagueKeywordDetector\n",
    "from utils import create_custom_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f59aa2-01be-479b-a987-af6fcfd9939c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load all trans from Matt's file\n",
    "trans = pd.read_csv(\"all_watch_observer_trans.csv\")\n",
    "trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35fe92c8-525f-43c0-8e0f-c325a5dc80dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pt_utts = (trans[\"Speaker\"] == \"Patient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51df790-6ad0-4d6b-8bbf-663d784788fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Filler speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63479305-0d4c-4897-85ba-1f6a36beee43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# init detector\n",
    "nlp = create_custom_nlp()\n",
    "filler_detector = FillerKeywordDetector(nlp)\n",
    "# apply detector to all patient utterances\n",
    "trans[\"filler_dets\"] = trans.loc[pt_utts, \"Transcript\"].apply(filler_detector.detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7986d1a0-0ec9-4290-8845-64670cb8096f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Repetitive speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f0d314-5803-40c2-b692-2d0ec2dd85a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# init detector\n",
    "repetition_detector = WordRepetitionDetector(nlp, window_size=2)\n",
    "# apply detector to all patient utterances\n",
    "trans[\"repetition_dets\"] = trans.loc[pt_utts, \"Transcript\"].apply(repetition_detector.detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2320fdee-b657-4cbc-9fd9-113defa876ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Substitution errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1188a35e-8881-4c76-b345-81ec674411bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import mlflow\n",
    "from openai import OpenAI\n",
    "from utils import llm_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97105d28-2424-4773-9b24-70dcba3f0150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = '''# INSTRUCTIONS\n",
    "You are a neurologist analyzing a patient's speech sample for signs of cognitive impairment. \n",
    "\n",
    "Your task is to identify all substitution errors in a patient's speech provided in the input below.\n",
    "\n",
    "### Definition\n",
    "Substitution errors occur when a person involuntarily replaces their intended word with an unintended word while speaking. Focus on detecting the following five substitution errors types:\n",
    "- Phonemic paraphasias, where sounds within the intended word are added, dropped, substituted, or rearranged (e.g., saying ``papple'' for ``apple''). \n",
    "- Semantic paraphasias, where the intended word is substituted entirely with another real word (e.g., saying ``cat'' for ``dog'').\n",
    "- Neologisms, where the entire intended word is substituted with a non-word (e.g., saying \"foundament\" for \"foundation\").\n",
    "- Morphological errors, where the intended word is used in the incorrect form, such as the wrong number (e.g., saying \"child\" for \"children\") or tense (e.g., saying \"walked\" for \"walk\").\n",
    "- Intra-word dysfluencies, where the production of the intended word is disrupted by an inserted sound (e.g., saying \"beuhcause\" for \"because\").\n",
    "\n",
    "Only flag single words that are clinically significant substitution errors and use the surrounding utterances to better understand the context of any given word.\n",
    "\n",
    "### Output Format\n",
    "Your output must be a single JSON object with a single key \"detections\" whose value is an array of JSON objects. Each object in the array represents one detected substitution error and must have the following keys-value pairs:\n",
    "- \"type\": \"substitution error\".\n",
    "- \"utt_num\": The number of the utterance in which the error occurs.\n",
    "- \"text\": The verbatim text of the substition error.\n",
    "- \"span\": The character span for the \"text\" in the \"utt_num\"-th utterance.\n",
    "- \"justification\": A brief explanation of why the \"text\" is a substitution error within its specific context.\n",
    "\n",
    "# INPUT\n",
    "{input_text}\n",
    "'''\n",
    "\n",
    "mlflow_creds = mlflow.utils.databricks_utils.get_databricks_host_creds()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=mlflow_creds.token,\n",
    "    base_url=f\"{mlflow_creds.host}/serving-endpoints\"\n",
    ")\n",
    "\n",
    "sub_err_detector = lambda text: llm_call(client, \"openai_gpt_4o\", None, prompt.format(input_text=text), {\"type\": \"json_object\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e060fd3b-cd92-4781-a6b0-e42394528d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "detections = {}\n",
    "with tqdm(total=trans[\"patient_id\"].unique().shape[0]) as pbar:\n",
    "    for idx, grp in trans.groupby(\"patient_id\"):\n",
    "        pt_transcript = \"\\n\".join(grp.index.astype(str) + \" \" + grp[\"Speaker\"].astype(str) + \": \" + grp[\"Transcript\"].astype(str))\n",
    "        try:\n",
    "            detections[idx] = sub_err_detector(pt_transcript)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        pbar.update(1)\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a4a19b-def9-4d1d-87ef-39cff3c66c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trans[\"sub_err_dets\"] = pd.NA\n",
    "for pt_id, dets in detections.items():\n",
    "    for d in dets[\"detections\"]:\n",
    "        cur_dets = trans.at[d[\"utt_num\"], \"sub_err_dets\"]\n",
    "\n",
    "        if pd.isna(cur_dets):\n",
    "            cur_dets = {\"detections\": []}\n",
    "        \n",
    "        cur_dets[\"detections\"].append(d)\n",
    "        trans.at[d[\"utt_num\"], \"sub_err_dets\"] = cur_dets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d729db33-0c32-45b6-98b2-660427b33ef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vague speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b593c3-b6dd-45bf-80c4-afafaf22189d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# init detector\n",
    "vague_detector = VagueKeywordDetector(nlp)\n",
    "# apply detector to all patient utterances\n",
    "trans[\"vague_dets\"] = trans.loc[pt_utts, \"Transcript\"].apply(vague_detector.detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7cf7c8-f448-45c3-8f37-39056113755c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aggregate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a56a7222-44cb-4dd2-bf12-eca2014f7be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_detection_rate(trans, output_name, pt_id_col_name, nlp):\n",
    "    num = trans.groupby(pt_id_col_name).apply(lambda grp: grp[output_name].apply(lambda x: len(x[\"detections\"]) if not pd.isna(x) else 0).sum())\n",
    "    den = trans.groupby(pt_id_col_name).apply(lambda grp: grp.apply(lambda x: sum([1 for token in nlp(x[\"Transcript\"]) if not (token.is_punct or token.is_space or token._.is_silence_tag or token._.is_inaudible_tag or token._.is_event_tag)]) if x[\"Speaker\"] == \"Patient\" else 0, axis=1).sum())\n",
    "    return 100 * num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d163fd-c309-40c2-8376-ac48a2ad8e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filler_rate = compute_detection_rate(trans, \"filler_dets\", \"patient_id\", nlp)\n",
    "repetition_rate = compute_detection_rate(trans, \"repetition_dets\", \"patient_id\", nlp)\n",
    "vague_term_rate = compute_detection_rate(trans, \"vague_dets\", \"patient_id\", nlp)\n",
    "sub_error_rate = compute_detection_rate(trans, \"sub_err_dets\", \"patient_id\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8826619512988156,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539733dc-5407-46a3-937f-4f16332d3457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8826619512988156,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a1cabe-702b-4c3f-8fe4-ecdbfe0971ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def inter_detection_distance(trans, output_name, pt_id_col_name, nlp, scaler=None, feat_name=\"IDD\"):\n",
    "    pt_ids = trans[pt_id_col_name].unique()\n",
    "    ifd_metrics = pd.DataFrame(index=pt_ids, columns=[f\"mean_{feat_name}\", f\"std_{feat_name}\", f\"mean_{feat_name}_norm\", f\"mean_{feat_name}_imputed\", f\"std_{feat_name}_imputed\"], dtype=float)\n",
    "\n",
    "    for pt_id, grp in trans.groupby(pt_id_col_name):\n",
    "        ifds = []\n",
    "        for idx, row in grp.iterrows():\n",
    "            if row[\"Speaker\"] == \"Patient\":\n",
    "                if not pd.isna(row[output_name]) and len(row[output_name][\"detections\"]) > 1:\n",
    "                    # get word spans for utterance\n",
    "                    doc = nlp(row[\"Transcript\"])\n",
    "                    word_spans = [(token.text, token.idx, token.idx + len(token.text)) for token in doc if not (token.is_punct or token.is_space or token._.is_silence_tag or token._.is_inaudible_tag or token._.is_event_tag)]\n",
    "                    # print(\"word_spans\", word_spans)\n",
    "\n",
    "                    # get filler words indices\n",
    "                    filler_word_idxs = []\n",
    "                    for det in row[output_name][\"detections\"]:\n",
    "                        try:\n",
    "                            filler_word_idxs.extend([word_spans.index((det[\"text\"], det[\"span\"][0], det[\"span\"][1]))])\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                    \n",
    "                    # filler_word_idxs = [word_spans.index((det[\"text\"], det[\"span\"][0], det[\"span\"][1])) for det in row[output_name][\"detections\"]]\n",
    "                    # print(\"filler_word_idxs:\", filler_word_idxs)\n",
    "\n",
    "                    # inter filler distance\n",
    "                    ifds.extend([filler_word_idxs[i+1] - filler_word_idxs[i] - 1 for i in range(len(filler_word_idxs) - 1)])\n",
    "                    # print(\"ifd\", ifds[-1])\n",
    "                    # break\n",
    "\n",
    "        ifd_metrics.loc[pt_id, f\"mean_{feat_name}\"] = np.mean(ifds)\n",
    "        ifd_metrics.loc[pt_id, f\"std_{feat_name}\"] = np.std(ifds)\n",
    "\n",
    "    ifd_metrics[f\"mean_{feat_name}_norm\"] = scaler.transform(ifd_metrics[[f\"mean_{feat_name}\"]])\n",
    "    ifd_metrics[f\"mean_{feat_name}_imputed\"] = ifd_metrics[f\"mean_{feat_name}_norm\"].fillna(1.0)\n",
    "    ifd_metrics[f\"std_{feat_name}_imputed\"] = ifd_metrics[f\"std_{feat_name}\"].fillna(0)\n",
    "\n",
    "\n",
    "    return ifd_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8826619512988156,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42f1c20-9d27-4ca2-8e59-6c20e62ccad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"IFD_scaler.pkl\", \"rb\") as f:\n",
    "    IFD_scaler = pickle.load(f)\n",
    "\n",
    "IFD = inter_detection_distance(trans, \"filler_dets\", \"patient_id\", nlp, scaler=IFD_scaler, feat_name=\"IFD\")\n",
    "\n",
    "with open(\"ISED_scaler.pkl\", \"rb\") as f:\n",
    "    ISED_scaler = pickle.load(f)\n",
    "\n",
    "ISED = inter_detection_distance(trans, \"sub_err_dets\", \"patient_id\", nlp, scaler=ISED_scaler, feat_name=\"ISED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8826619512988156,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd48b65-f58b-4b47-a809-4bea2fc508db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def detected_utterances_ratio(trans, output_name, pt_id_col_name):\n",
    "    num = trans.groupby(pt_id_col_name).apply(lambda grp: grp[output_name].apply(lambda x: len(x[\"detections\"]) > 0 if not pd.isna(x) else False).sum())\n",
    "    den = trans.groupby(pt_id_col_name).apply(lambda grp: grp.shape[0])\n",
    "    return 100 * num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8826619512988156,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994e281b-dc24-401f-9526-3394bda280d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vague_utt_ratio = detected_utterances_ratio(trans, \"vague_dets\", \"patient_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8826619512988156,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60053b74-5b4f-4be6-95da-449796f30f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "POS = [\n",
    "    \"ADJ\",      # adjective\n",
    "    \"ADP\",      # adposition\n",
    "    \"ADV\",      # adverb\n",
    "    \"AUX\",      # auxiliary\n",
    "    \"CCONJ\",    # coordinating conjunction\n",
    "    \"DET\",      # determiner\n",
    "    \"INTJ\",     # interjection\n",
    "    \"NOUN\",     # noun\n",
    "    \"NUM\",      # numeral\n",
    "    \"PART\",     # particle\n",
    "    \"PRON\",     # pronoun\n",
    "    \"PROPN\",    # proper noun\n",
    "    # \"PUNCT\",    # punctuation\n",
    "    \"SCONJ\",    # subordinating conjunction\n",
    "    # \"SYM\",      # symbol\n",
    "    \"VERB\",     # verb\n",
    "    \"X\",        # other\n",
    "    # \"SPACE\",    # space\n",
    "]\n",
    "\n",
    "def pos_repetition_counts(trans, output_name, pt_id_col_name):\n",
    "    pt_ids = trans[pt_id_col_name].unique()\n",
    "    pos_metrics = pd.DataFrame(index=pt_ids, columns=POS, dtype=float)\n",
    "\n",
    "    for pt_id in pos_metrics.index:\n",
    "        counts = dict.fromkeys(POS, 0)\n",
    "        num_words = 0\n",
    "        for utt_num, row in trans.loc[trans[pt_id_col_name] == pt_id].iterrows():\n",
    "            if row[\"Speaker\"] == \"Patient\":\n",
    "                num_words += sum([1 for token in nlp(row[\"Transcript\"]) if not (token.is_punct or token.is_space or token._.is_silence_tag or token._.is_inaudible_tag or token._.is_event_tag)])\n",
    "\n",
    "                if row[output_name][\"detections\"]:\n",
    "                    for det in row[output_name][\"detections\"]:\n",
    "                        doc = nlp(det[\"text1\"])\n",
    "                        try:\n",
    "                            counts[doc[0].pos_] += 1                              \n",
    "                        except KeyError as e:\n",
    "                            print(e)\n",
    "                            print(row[\"Transcript\"])\n",
    "                            print(det)\n",
    "\n",
    "        for pos in POS:\n",
    "            pos_metrics.loc[pt_id, pos] = 100 * counts[pos] / num_words\n",
    "\n",
    "    return pos_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8826619512988156,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fbafc28-1d2f-4761-a471-a5c932488f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pos_rep_rates = pos_repetition_counts(trans, \"repetition_dets\", \"patient_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b651507-3258-44aa-9708-72d3cc2dd1bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Compile into feature file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8826619512988156,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1393460-08c0-4d5f-9342-11eeb3e297f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features = pd.concat([filler_rate, repetition_rate, sub_error_rate, vague_term_rate, vague_utt_ratio], keys=[\"filler_rate\", \"repetition_rate\", \"sub_err_rate\", \"vague_term_rate\", \"vague_utt_ratio\"], axis=1) \n",
    "features = pd.concat([features, IFD[[\"mean_IFD_imputed\", \"std_IFD_imputed\"]], ISED[[\"mean_ISED_imputed\", \"std_ISED_imputed\"]]], axis=1)\n",
    "features = pd.concat([features, pos_rep_rates.add_suffix('_repetition_rate')], axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8826619512988156,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f62d64-92a6-4cc6-a635-9db42ef9eda2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features.to_csv(\"OBSERVER_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fe94b6-0796-439a-87a8-88f04b4c81b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trans.to_pickle(\"OBSERVER_trans_with_dets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca290651-06a9-417d-9358-07f5268fa7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_External_Validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
